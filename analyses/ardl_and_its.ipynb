{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f88b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "\n",
    "from utils import *\n",
    "from analysis_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Necessary data (both weekly and monthly):\n",
    "- Shocks\n",
    "- Confounds\n",
    "- Trends outcomes\n",
    "- KSU\n",
    "\"\"\"\n",
    "\n",
    "prefix = '../data/prepared/merged/'\n",
    "units = 'months'\n",
    "\n",
    "if units == 'weeks':\n",
    "    bonf_denom = 7\n",
    "elif units == 'months':\n",
    "    bonf_denom = 6\n",
    "else:\n",
    "    assert 0\n",
    "\n",
    "do_controls = False\n",
    "do_primary = True\n",
    "\n",
    "# Run primary analyses\n",
    "pval_thresh_1 = 0.05/bonf_denom\n",
    "\n",
    "month_tests = 6*15*3 #intvns * outcomes * [assoc, contemp, lagged]\n",
    "week_tests = 7*12*3 # intvns * outcomes * [assoc, contemp lagged]\n",
    "\n",
    "all_tests = month_tests + week_tests\n",
    "\n",
    "pval_thresh_2 = 0.05/all_tests#(month_tests + week_tests)\n",
    "\n",
    "# Settings for non-bin models\n",
    "do_differencing = True\n",
    "include_time = False\n",
    "\n",
    "# Settings for both\n",
    "add_L = True\n",
    "normalize = True\n",
    "include_month = True\n",
    "\n",
    "# Settings for bin models\n",
    "do_differencing_bin = False\n",
    "include_time_bin = True\n",
    "duration_months = 5\n",
    "bin_slope = True\n",
    "\n",
    "# KSU params\n",
    "ksu_lag = None\n",
    "\n",
    "verbose=True\n",
    "models = ['assoc', 'contemp', 'lagged', 'bin']\n",
    "\n",
    "\n",
    "netflix_release_dates = {'tgc': [2019, 10, 16], 'fok': [2011, 5, 6], 'okja': [2017, 6, 28],\n",
    "                'wth': [2017, 6, 16], 'cowspiracy': [2015, 9, 15], 'owth': [2017, 6, 16], \n",
    "                         'yawye': [2024, 1, 1]}\n",
    "\n",
    "restrictions = {'StewartMilk': ['cowspiracy', 'wth', 'okja', 'owth', 'all_docs'], \n",
    "                'StewartPBMilk': ['cowspiracy', 'wth', 'okja', 'owth', 'all_docs'],\n",
    "                'Zhao': ['wth', 'okja', 'owth', 'tgc', 'all_docs']}\n",
    "\n",
    "binary_analysis_dct = {'Zhao': ['owth', 'wth', 'okja', 'tgc'], 'NeuhoferLusk': ['tgc'],\n",
    "                      'StewartMilk': ['cowspiracy', 'owth', 'wth', 'okja'], \n",
    "                       'StewartPBMilk': ['cowspiracy', 'owth', 'wth', 'okja']}\n",
    "\n",
    "normalized = pd.read_csv(prefix + 'merged_' + units + '.csv')\n",
    "\n",
    "if do_controls:\n",
    "    normalized_controls = pd.read_csv(prefix + 'merged_controls_' + units + '.csv')\n",
    "\n",
    "assert units in ['weeks', 'months', 'days']\n",
    "assert len(set(units) & set(prefix)) !=0 \n",
    "\n",
    "#Lists\n",
    "common = ['ds', 'Time', 'Month']\n",
    "\n",
    "all_docs = ['tgc', 'wth', 'fok', 'cowspiracy', 'okja', 'yawye']\n",
    "all_ts_albums = ['reputation', 'ts_1989', 'lover', 'speak_now', 'red']\n",
    "all_climate = ['climate', 'climate_change', 'sustainability']\n",
    "all_ts_outcomes = ['taylor_swift', 'taylor_swift_lyrics', 'taylor_swift_songs',\n",
    "                  'taylor_swift_tour', 'taylor_swift_album']\n",
    "\n",
    "all_drake_albums = ['scorpion', 'take_care', 'views', 'nwts', 'tml']\n",
    "all_drake_outcomes = ['drake', 'drake_lyrics', 'drake_songs', 'drake_tour', 'drake_album']\n",
    "\n",
    "\n",
    "if units == 'months':\n",
    "    all_primary_outcomes = ['plant_based_plus_plant_based', 'vegan', 'vegetarian', \n",
    "                           'ksu_beef', 'ksu_pork', 'ksu_chicken']\n",
    "    \n",
    "    all_primary_outcomes += ['StewartMilk',  \n",
    "                            'StewartPBMilk', 'Zhao'] \n",
    "\n",
    "    all_secondary_outcomes = ['vegan_informative', 'vegetarian_informative',\n",
    "           'plant_based_informative', 'vegan_behavior', 'vegetarian_behavior',\n",
    "           'plant_based_behavior']\n",
    "    \n",
    "    all_outcomes = all_primary_outcomes + all_secondary_outcomes \n",
    "    \n",
    "    intvns = ['fok', 'cowspiracy', 'owth', 'tgc', 'yawye', 'all_docs']\n",
    "    other_docs = ['fok', 'cowspiracy', 'owth', 'tgc', 'yawye']\n",
    "    \n",
    "elif units == 'weeks':\n",
    "    all_primary_outcomes = ['plant_based_plus_plant_based','vegan', 'vegetarian',\n",
    "                            'StewartMilk', \n",
    "                            'StewartPBMilk',\n",
    "                            'Zhao']   \n",
    "\n",
    "    all_secondary_outcomes = ['vegan_informative', 'vegetarian_informative',\n",
    "           'plant_based_informative', 'vegan_behavior', 'vegetarian_behavior',\n",
    "           'plant_based_behavior']\n",
    "    \n",
    "    all_outcomes = all_primary_outcomes + all_secondary_outcomes\n",
    "    intvns = ['fok', 'cowspiracy', 'okja', 'wth', 'tgc', 'yawye', 'all_docs'] #'okja', 'wth'        \n",
    "    \n",
    "    other_docs = ['fok', 'cowspiracy', 'okja', 'wth', 'tgc', 'yawye'] #'okja', 'wth'\n",
    "    \n",
    "    if model == 'bin':\n",
    "        intvns = ['fok', 'cowspiracy', 'owth', 'tgc', 'yawye', 'all_docs']\n",
    "        other_docs = ['fok', 'cowspiracy', 'owth', 'tgc', 'yawye']\n",
    "\n",
    "else:\n",
    "    assert 0\n",
    "        \n",
    "# Outcomes, interventions, and models to run\n",
    "test_outcomes = all_outcomes\n",
    "test_intvns =  intvns\n",
    "models_to_run = ['assoc', 'contemp', 'lagged']\n",
    "\n",
    "for model in models_to_run:\n",
    "    assert model in models\n",
    "\n",
    "#run_name = 'bin_{num}months_slope{val}'.format(num=duration_months, val=bin_slope)\n",
    "\n",
    "run_name = 'test'#'ksu_lag_' + str(ksu_lag) + 'assoc_contemp_lagged_2024'\n",
    "\n",
    "print(run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618bbfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift KSU\n",
    "if ksu_lag:\n",
    "    normalized['ksu_chicken'] = normalized['ksu_chicken'].shift(-1*ksu_lag)\n",
    "    normalized['ksu_pork'] = normalized['ksu_pork'].shift(-1*ksu_lag)\n",
    "    normalized['ksu_beef'] = normalized['ksu_beef'].shift(-1*ksu_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_analysis_dct(df, X, Y, trt_lag, model, add_L, include_time, include_month, confounds, non_add_L_set):\n",
    "    analysis_dct = {'C': [], 'Ind_PS': []}\n",
    "    \n",
    "    if include_time:\n",
    "        #df['Time**2'] = df['Time']*df['Time']\n",
    "        analysis_dct['C'].append(('Time', [0]))\n",
    "        analysis_dct['Ind_PS'].append(('Time', [0]))\n",
    "        #analysis_dct['C'].append(('Time**2', [0]))\n",
    "        #analysis_dct['Ind_PS'].append(('Time**2', [0]))\n",
    "        \n",
    "    if include_month:\n",
    "        month_dummies = []\n",
    "        month_vals = sorted(df['Month'].unique())\n",
    "        for val_idx in range(len(month_vals)):\n",
    "            if val_idx == 0:\n",
    "                continue\n",
    "            this_val = month_vals[val_idx]\n",
    "            df['Month_Dummy' + str(this_val)] = (df['Month'] == this_val).astype(float)\n",
    "            month_dummies.append('Month_Dummy' + str(this_val))\n",
    "        \n",
    "        \n",
    "            analysis_dct['C'].append(('Month_Dummy' + str(this_val), [0]))\n",
    "            analysis_dct['Ind_PS'].append(('Month_Dummy' + str(this_val), [0]))\n",
    "    \n",
    "    \n",
    "    if model == 'lagged':\n",
    "        #\"\"\"\n",
    "        analysis_dct['X'] = (X, [trt_lag,trt_lag+1])\n",
    "        analysis_dct['Y'] = (Y, [trt_lag+1])\n",
    "         \n",
    "        \n",
    "        for c in confounds:\n",
    "            if add_L and c not in non_add_L_set:\n",
    "                analysis_dct['C'].append((c, [trt_lag,trt_lag+1]))\n",
    "            else:\n",
    "                analysis_dct['C'].append((c, [trt_lag+1]))\n",
    "                \n",
    "                \n",
    "        analysis_dct['Dep_PS'] = (X, [0])\n",
    "        analysis_dct['Ind_PS'] += [(X, [1]), (Y, [1])]\n",
    "\n",
    "        for c in confounds:\n",
    "            analysis_dct['Ind_PS'].append((c, [1]))          \n",
    "            if add_L:\n",
    "                analysis_dct['Ind_PS'].append((c, [0]))   \n",
    "\n",
    "    elif model == 'contemp':\n",
    "        analysis_dct['X'] = (X, [0,1])\n",
    "        analysis_dct['Y'] = (Y, [1]) \n",
    "        \n",
    "        for c in confounds:\n",
    "            if add_L and c not in non_add_L_set:\n",
    "                analysis_dct['C'].append((c, [0,1]))\n",
    "            else:\n",
    "                analysis_dct['C'].append((c, [1]))  \n",
    "                \n",
    "        analysis_dct['Dep_PS'] = (X, [0])\n",
    "        analysis_dct['Ind_PS'] += [(X, [1]), (Y, [1])]\n",
    "\n",
    "        for c in confounds:\n",
    "            analysis_dct['Ind_PS'].append((c, [1]))          \n",
    "            if add_L:\n",
    "                analysis_dct['Ind_PS'].append((c, [0]))                 \n",
    "\n",
    "\n",
    "    elif model == 'assoc':\n",
    "        analysis_dct['Dep_PS'] = None\n",
    "        analysis_dct['Ind_PS'] = None\n",
    "        analysis_dct['X'] = (X, [0])\n",
    "        analysis_dct['Y'] = (Y, []) \n",
    "    \n",
    "    else:\n",
    "        assert 0\n",
    "    \n",
    "    return df, analysis_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run controls\n",
    "\n",
    "\"\"\"\n",
    "Positive and negative controls.\n",
    "Positive controls: (sum of TS albums) x each TS outcome\n",
    "Negative controls: \n",
    "1) (each TS album + sum) x vegan recipes\n",
    "2) (each doc + sum) x TS outcomes\n",
    "\n",
    "Binary approach: \n",
    "Y_t ~ I_t + L_t + L_t-1 + Y_t-1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def run_controls(df, pval_thresh=0.05, PS=False, PS_logistic=False, fit_method='OLS', model='lagged', bonf_denom=1,\n",
    "                add_L=True, difference=False, normalize=False, include_month=True, include_time=True,\n",
    "                verbose=False):\n",
    "    assert model in ['assoc', 'lagged', 'contemp']\n",
    "    if model == 'assoc':\n",
    "        assert add_L is False\n",
    "\n",
    "    # Positive controls #1\n",
    "    pval_thresh = 0.05/bonf_denom\n",
    "    total = 0\n",
    "    passed = 0\n",
    "    \n",
    "    trt_lag = 1 if (model == 'lagged') else 0\n",
    "    non_add_L_set = []\n",
    "    confounds = []\n",
    "    \n",
    "    for X in ['all_ts_albums']:\n",
    "        for Y in all_ts_outcomes:\n",
    "            df, analysis_dct = prepare_analysis_dct(df.copy(), X, Y, \n",
    "                                                    trt_lag, \n",
    "                                                    model, \n",
    "                                                    add_L, include_time, \n",
    "                                                    include_month, \n",
    "                                                    confounds, non_add_L_set)\n",
    "            \n",
    "            x_pval, x_beta, _, _ = run_analysis(analysis_dct, df.copy(), trt_lag, non_add_L_set,\n",
    "                                                PS=PS, PS_logistic=PS_logistic,\n",
    "                                          fit_method=fit_method,\n",
    "                                         difference=difference, normalize=normalize,\n",
    "                                               verbose=verbose, include_time=False,\n",
    "                                               include_month=False)\n",
    "\n",
    "            #print(X, Y, x_pval)\n",
    "            if x_pval < pval_thresh:\n",
    "                passed += 1\n",
    "            total += 1\n",
    "    print('TS Pos Control tests success fraction: ', float(passed)/total, 'Passed: ', passed, 'Total: ', total)\n",
    " \n",
    "    # Positive controls #2\n",
    "    total = 0\n",
    "    passed = 0\n",
    "    \n",
    "    for X in ['all_drake_albums']:\n",
    "        for Y in all_drake_outcomes:\n",
    "            df, analysis_dct = prepare_analysis_dct(df.copy(), X, Y, \n",
    "                                                    trt_lag, \n",
    "                                                    model, \n",
    "                                                    add_L, include_time, \n",
    "                                                    include_month, \n",
    "                                                    confounds, non_add_L_set)                \n",
    "            #for alb in all_ts_albums:\n",
    "            #    analysis_dct['C'].append((alb, 2))\n",
    "            x_pval, x_beta, _, _ = run_analysis(analysis_dct, df.copy(), trt_lag, non_add_L_set,\n",
    "                                                PS=PS,\n",
    "                                          PS_logistic=PS_logistic,\n",
    "                                          fit_method=fit_method,\n",
    "                                         difference=difference, normalize=normalize,\n",
    "                                               verbose=verbose, include_time=False,\n",
    "                                               include_month=False)\n",
    "\n",
    "            #print(X, Y, x_pval)\n",
    "\n",
    "            if x_pval < pval_thresh:\n",
    "                passed += 1\n",
    "            total += 1\n",
    "    print('Drake Pos Control tests success fraction: ', float(passed)/total, 'Passed: ', passed, 'Total: ', total)\n",
    "\n",
    "\n",
    "    # Negative controls #1\n",
    "    total1 = 0\n",
    "    passed1 = 0\n",
    "    \n",
    "    X_vals = all_ts_albums + ['all_ts_albums'] + all_drake_albums + ['all_drake_albums']\n",
    "    confounds = ['health', 'animal_welfare', 'all_climate']\n",
    "    for X in X_vals:\n",
    "        for Y in all_primary_outcomes:\n",
    "            df, analysis_dct = prepare_analysis_dct(df.copy(), X, Y, \n",
    "                                                    trt_lag, \n",
    "                                                    model, \n",
    "                                                    add_L, include_time, \n",
    "                                                    include_month, \n",
    "                                                    confounds, non_add_L_set)  \n",
    "                   \n",
    "            x_pval, x_beta, _, _ = run_analysis(analysis_dct, df.copy(), trt_lag, \n",
    "                                                non_add_L_set,\n",
    "                                                PS=PS, \n",
    "                                          PS_logistic=PS_logistic,\n",
    "                                          fit_method=fit_method,\n",
    "                                         difference=difference, normalize=normalize,\n",
    "                                               verbose=verbose, include_time=False,\n",
    "                                               include_month=False)\n",
    "            if x_pval >= pval_thresh:\n",
    "                passed1 += 1\n",
    "            else:\n",
    "                print('failed')\n",
    "                print(X, Y, x_pval)\n",
    "            total1 += 1\n",
    "    print('Neg Control #1 tests success fraction: ', float(passed1)/total1, 'Passed: ', passed1, 'Total: ', total1)\n",
    "    \n",
    "    # Negative controls #2\n",
    "    # Add in the other docs as precision covariates?\n",
    "    total2 = 0\n",
    "    passed2 = 0\n",
    "    \n",
    "    \n",
    "    Y_vals = all_ts_outcomes + all_drake_outcomes\n",
    "    for X in all_docs + ['all_docs']:\n",
    "        for Y in Y_vals:\n",
    "            \n",
    "            df, analysis_dct = prepare_analysis_dct(df.copy(), X, Y, \n",
    "                                                    trt_lag, \n",
    "                                                    model, \n",
    "                                                    add_L, include_time, \n",
    "                                                    include_month, \n",
    "                                                    confounds, non_add_L_set)  \n",
    "            \n",
    "            x_pval, x_beta, _, _ = run_analysis(analysis_dct, df.copy(), trt_lag, \n",
    "                                                non_add_L_set,\n",
    "                                                PS=PS, \n",
    "                                          PS_logistic=PS_logistic,\n",
    "                                          fit_method=fit_method,\n",
    "                                         difference=difference, normalize=normalize,\n",
    "                                               verbose=verbose, include_time=False,\n",
    "                                               include_month=False)\n",
    "\n",
    "            if x_pval >= pval_thresh:\n",
    "                passed2 += 1\n",
    "            else:\n",
    "                print('failed')\n",
    "                print(X, Y, x_pval)\n",
    "            total2 += 1\n",
    "            \n",
    "    print('Neg Control #2 success fraction: ', float(passed2)/total2, 'Passed: ',  passed2, 'Total: ', total2)\n",
    "    print('Neg Control success fraction: ', float(passed1 + passed2)/(total1 + total2), 'Passed: ',  passed1 + passed2, 'Total: ', total1 + total2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2dc928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models: assoc, lagged, contemp, bin.\n",
    "Assoc: Y_t ~ X_t\n",
    "Contemp: Y_t ~ PS_t + X_t + X_t-1 + Y_t-1 + L_t + L_t-1 + Month\n",
    "Lagged: Y_t ~ PS_t-1 + X_t-1 + X_t-2 + L_t-1 + L_t-2 + Y_t-2 + Month\n",
    "Bin: Y_t ~ PS + I_t + I_t*Time + Time + L_t + Month #+ Y_t-1 \n",
    "\"\"\"\n",
    "def run_primary_analyses(df, X, outcome, PS=False, PS_logistic=False, fit_method='OLS', model='lagged', \n",
    "                         add_L=True, difference=False, include_time=False, \n",
    "                         include_month=False, duration_months=None, normalize=False, verbose=False):\n",
    "    assert model in ['assoc', 'lagged', 'contemp', 'bin']\n",
    "    if model is 'assoc':\n",
    "        assert add_L is False\n",
    "        assert PS is False\n",
    "\n",
    "    if units == 'months':\n",
    "        other_docs = ['fok', 'cowspiracy', 'owth', 'tgc']\n",
    "\n",
    "    elif units == 'weeks':\n",
    "        other_docs = ['fok', 'cowspiracy', 'okja', 'wth', 'tgc']\n",
    "    elif units == 'days':\n",
    "        other_docs = ['fok', 'cowspiracy', 'okja', 'wth']\n",
    "    else:\n",
    "        assert 0\n",
    "    bin_other_docs = {'fok': [], 'cowspiracy': ['fok'], 'owth': ['fok', 'cowspiracy'],\n",
    "                      'okja': ['fok', 'cowspiracy'], 'wth': ['fok', 'cowspiracy'],\n",
    "                         'tgc': ['fok', 'cowspiracy', 'owth'],\n",
    "                     'yawye': ['fok', 'cowspiracy', 'owth', 'tgc']}\n",
    "        \n",
    "    if duration_months is not None:\n",
    "        assert model == 'bin'\n",
    "        init_date = df['ds'][0]\n",
    "        this_date = '{y}-{m}-{d}'.format(y=netflix_release_dates[X][0], \n",
    "                                             m=netflix_release_dates[X][1], d=netflix_release_dates[X][2])\n",
    "\n",
    "        release_date_time = date_difference(init_date, this_date, units)\n",
    "        \n",
    "        if units == 'months':\n",
    "            offset = duration_months\n",
    "        elif units == 'weeks':\n",
    "            offset = duration_months*4\n",
    "        else:\n",
    "            assert 0\n",
    "        df = df.loc[df['Time'] <= release_date_time + offset]\n",
    "        \n",
    "        print('Intvn start time: ', df.loc[df[X + '_bin'] == 1].head()['ds'])\n",
    "        print('After truncating by duration_months: ', df.tail()['ds'])\n",
    "        \n",
    "    \"\"\"\n",
    "    if combine_owth:\n",
    "        disaggr_docs = list(set(all_docs) - set(['okja', 'wth'])) + ['owth']\n",
    "    else:\n",
    "        disaggr_docs = list(set(all_docs)) \n",
    "    \"\"\"\n",
    "        \n",
    "    trt_lag = 1 if (model == 'lagged') else 0\n",
    "    #trt_lag = 2 if (model == 'lagged') else 0\n",
    "    \n",
    "    confounds = ['health', 'animal_welfare', 'all_climate']\n",
    "    trends_full_additional_confounds = ['food']\n",
    "    trends_inf_additional_confounds = ['informative_bare']\n",
    "    trends_behav_additional_confounds = ['behavior_bare']\n",
    "    consumption_additional_confounds = ['rdpi']\n",
    "    non_add_L_set = []\n",
    "\n",
    "    \n",
    "    category_dct = {'vegan': 'trends_full', 'vegetarian': 'trends_full', \n",
    "                    'plant_based_plus_plant_based': 'trends_full', \n",
    "                    'vegan_informative': 'trends_inf', 'vegetarian_informative': 'trends_inf',\n",
    "       'plant_based_informative': 'trends_inf', \n",
    "                    'vegan_behavior': 'trends_behav', 'vegetarian_behavior': 'trends_behav',\n",
    "       'plant_based_behavior': 'trends_behav',\n",
    "                       'ksu_chicken': 'consumption', 'ksu_pork': 'consumption', \n",
    "                    'ksu_beef': 'consumption', 'Zhao': 'consumption', 'NeuhoferLusk': 'consumption',\n",
    "                   'StewartMilk': 'consumption', 'StewartPBMilk': 'consumption'}\n",
    "    \n",
    "    if category_dct[outcome] == 'trends_full':\n",
    "        confounds += trends_full_additional_confounds\n",
    "    elif category_dct[outcome] == 'trends_inf':\n",
    "        confounds += trends_inf_additional_confounds\n",
    "    elif category_dct[outcome] == 'trends_behav':\n",
    "        confounds += trends_behav_additional_confounds        \n",
    "    else:\n",
    "        assert category_dct[outcome] == 'consumption'\n",
    "        confounds += consumption_additional_confounds\n",
    "            \n",
    "    #for X in disaggr_docs + ['all_docs']:\n",
    "    #, \n",
    "    \n",
    "    analysis_dct = {'C': [], 'Ind_PS': []}\n",
    "    \n",
    "    if include_time:\n",
    "        #df['Time**2'] = df['Time']*df['Time']\n",
    "        analysis_dct['C'].append(('Time', [0]))\n",
    "        analysis_dct['Ind_PS'].append(('Time', [0]))\n",
    "        #analysis_dct['C'].append(('Time**2', [0]))\n",
    "        #analysis_dct['Ind_PS'].append(('Time**2', [0]))\n",
    "        \n",
    "    if include_month:\n",
    "        month_dummies = []\n",
    "        month_vals = sorted(df['Month'].unique())\n",
    "        for val_idx in range(len(month_vals)):\n",
    "            if val_idx == 0:\n",
    "                continue\n",
    "            this_val = month_vals[val_idx]\n",
    "            df['Month_Dummy' + str(this_val)] = (df['Month'] == this_val).astype(float)\n",
    "            month_dummies.append('Month_Dummy' + str(this_val))\n",
    "        \n",
    "        \n",
    "            analysis_dct['C'].append(('Month_Dummy' + str(this_val), [0]))\n",
    "            analysis_dct['Ind_PS'].append(('Month_Dummy' + str(this_val), [0]))\n",
    "    \n",
    "    \n",
    "    if model == 'lagged':\n",
    "        #\"\"\"\n",
    "        analysis_dct['X'] = (X, [trt_lag,trt_lag+1])\n",
    "        analysis_dct['Y'] = (outcome, [trt_lag+1])\n",
    "         \n",
    "        \n",
    "        for c in confounds:\n",
    "            if add_L and c not in non_add_L_set:\n",
    "                analysis_dct['C'].append((c, [trt_lag,trt_lag+1]))\n",
    "            else:\n",
    "                analysis_dct['C'].append((c, [trt_lag+1]))\n",
    "                \n",
    "                \n",
    "        analysis_dct['Dep_PS'] = (X, [0])\n",
    "        analysis_dct['Ind_PS'] += [(X, [1]), (outcome, [1])]\n",
    "\n",
    "        for c in confounds:\n",
    "            analysis_dct['Ind_PS'].append((c, [1]))          \n",
    "            if add_L:\n",
    "                analysis_dct['Ind_PS'].append((c, [0]))   \n",
    "\n",
    "    elif model == 'contemp':\n",
    "        analysis_dct['X'] = (X, [0,1])\n",
    "        analysis_dct['Y'] = (outcome, [1]) \n",
    "        \n",
    "        for c in confounds:\n",
    "            if add_L and c not in non_add_L_set:\n",
    "                analysis_dct['C'].append((c, [0,1]))\n",
    "            else:\n",
    "                analysis_dct['C'].append((c, [1]))  \n",
    "                \n",
    "        analysis_dct['Dep_PS'] = (X, [0])\n",
    "        analysis_dct['Ind_PS'] += [(X, [1]), (outcome, [1])]\n",
    "\n",
    "        for c in confounds:\n",
    "            analysis_dct['Ind_PS'].append((c, [1]))          \n",
    "            if add_L:\n",
    "                analysis_dct['Ind_PS'].append((c, [0]))                 \n",
    "\n",
    "\n",
    "    elif model == 'assoc':\n",
    "        analysis_dct['X'] = (X, [0])\n",
    "        analysis_dct['Y'] = (outcome, [])   \n",
    "        \n",
    "    elif model == 'bin':\n",
    "        if not bin_slope:\n",
    "            #(X + '_bin*Time', [0])\n",
    "            analysis_dct['X'] = (X + '_bin', [0])\n",
    "            analysis_dct['Y'] = (outcome, [])\n",
    "            analysis_dct['C'].append((X + '_bin*Time_C', [0]))  \n",
    "            if add_L:\n",
    "                for c in confounds:\n",
    "                    analysis_dct['C'].append((c, [0]))\n",
    "                                        \n",
    "        else:\n",
    "            #(X + '_bin*Time', [0])\n",
    "            analysis_dct['X'] = (X + '_bin*Time_C', [0])\n",
    "            analysis_dct['Y'] = (outcome, [])\n",
    "            analysis_dct['C'].append((X + '_bin', [0]))   \n",
    "            if add_L:\n",
    "                for c in confounds:\n",
    "                    analysis_dct['C'].append((c, [0]))            \n",
    "                \n",
    "        # Specifically the PS regression\n",
    "        df[X + '_bin_orig'] = df[X + '_bin'].copy()\n",
    "        analysis_dct['Dep_PS'] = (X + '_bin_orig', [0])\n",
    "        analysis_dct['Ind_PS'] = []\n",
    "        if add_L:\n",
    "            for c in confounds:\n",
    "                analysis_dct['Ind_PS'].append((c, [0]))            \n",
    "    \n",
    "    # Add in confounds\n",
    "    if X != 'all_docs':\n",
    "        for other_doc in other_docs:\n",
    "            if other_doc == X:\n",
    "                continue\n",
    "            if model == 'lagged':\n",
    "                if add_L:\n",
    "                    analysis_dct['C'].append((other_doc, [trt_lag,trt_lag+1]))\n",
    "                else:\n",
    "                    analysis_dct['C'].append((other_doc, [trt_lag+1]))                    \n",
    "            elif model == 'contemp':\n",
    "                if add_L:\n",
    "                    analysis_dct['C'].append((other_doc, [0,1]))\n",
    "                else:\n",
    "                    analysis_dct['C'].append((other_doc, [1]))                \n",
    "            elif model == 'bin':\n",
    "                if other_doc in bin_other_docs[X]:\n",
    "                    if add_L:\n",
    "                        analysis_dct['C'].append((other_doc, [0]))\n",
    "            elif model == 'assoc':\n",
    "                pass\n",
    "\n",
    "\n",
    "    #print('analysis_dct: ', analysis_dct)\n",
    "            \n",
    "    x_pval, x_beta, x_se, _ = run_analysis(analysis_dct, df, trt_lag, non_add_L_set, \n",
    "                                        PS=PS, PS_logistic=PS_logistic, fit_method=fit_method,\n",
    "                                  add_L=add_L,\n",
    "                                  difference=difference,\n",
    "                                  include_time=False,\n",
    "                                  include_month=False,\n",
    "                                  normalize=normalize,\n",
    "                                 verbose=verbose)          \n",
    "\n",
    "    return x_pval, x_beta, x_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037438d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge for controls\n",
    "# 522\n",
    "if do_controls:\n",
    "    merged_for_controls = normalized.merge(normalized_controls, on=common)\n",
    "    run_controls(merged_for_controls.copy(), pval_thresh=0.05, PS=True, PS_logistic=False, \n",
    "                 fit_method='GLSAR', model='lagged', bonf_denom=522,\n",
    "                    add_L=False, difference=True, normalize=True, include_month=True,\n",
    "                include_time=False, verbose=False)\n",
    "    \n",
    "    \"\"\"\n",
    "    run_controls_binary(merged_for_controls.copy(), PS=False, PS_logistic=False, fit_method='GLSAR', model='bin', bonf_denom=5,\n",
    "                 add_L=True,\n",
    "                duration_months=12,\n",
    "                difference=False, verbose=False)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dfs for heatmaps\n",
    "heat_dfs = {}\n",
    "annot_dfs = {}\n",
    "\n",
    "intvn_map = {'fok': 'FOK', 'cowspiracy': 'Cowspiracy', 'wth': 'WTH', 'okja': \"Okja\", 'owth': \n",
    "             'Okja/WTH', 'tgc': 'TGC', 'yawye': 'YAWYE', 'all_docs': 'All'}\n",
    "outcome_map = {'vegan': \"Searches: `Vegan'\", 'vegetarian': \"Searches: `Vegetarian'\", \n",
    "               'plant_based_plus_plant_based': \"Searches: `Plant based'\",\n",
    "              'ksu_beef': 'Beef Demand',\n",
    "              'ksu_chicken': 'Chicken Demand',\n",
    "              'ksu_pork': 'Pork Demand',\n",
    "              'Zhao': 'Zhao',\n",
    "              'NeuhoferLusk': 'NeuhoferLusk',\n",
    "              'StewartPBMilk': 'StewartPBMilk',\n",
    "              'StewartMilk': 'StewartMilk',\n",
    "               'vegan_informative': \"Searches: `Vegan', Informative\",\n",
    "           'vegetarian_informative': \"Searches: `Vegetarian', Informative\", \n",
    "               'plant_based_informative': \"Searches: `Plant based', Informative\", \n",
    "               'vegan_behavior': \"Searches: `Vegan', Behavior\",\n",
    "           'vegetarian_behavior': \"Searches: `Vegetarian', Behavior\", \n",
    "               'plant_based_behavior': \"Searches: `Plant based', Behavior\"}\n",
    "\n",
    "coef_dcts = {}\n",
    "annot_dcts = {}\n",
    "pval_dcts = {}\n",
    "se_dcts = {}\n",
    "\n",
    "for model in models:\n",
    "    coef_dcts[model] = {}\n",
    "    annot_dcts[model] = {}\n",
    "    pval_dcts[model] = {}\n",
    "    se_dcts[model] = {}\n",
    "\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    for intvn in intvns:\n",
    "        coef_dcts[model][intvn_map[intvn]] = {}\n",
    "        annot_dcts[model][intvn_map[intvn]] = {}\n",
    "        pval_dcts[model][intvn_map[intvn]] = {}\n",
    "        se_dcts[model][intvn_map[intvn]] = {}\n",
    "        for outcome in all_outcomes:\n",
    "            coef_dcts[model][intvn_map[intvn]][outcome_map[outcome]] = np.nan\n",
    "            annot_dcts[model][intvn_map[intvn]][outcome_map[outcome]] = ''\n",
    "            pval_dcts[model][intvn_map[intvn]][outcome_map[outcome]] = np.nan\n",
    "            se_dcts[model][intvn_map[intvn]][outcome_map[outcome]] = np.nan\n",
    "    heat_dfs[model] = pd.DataFrame(coef_dcts[model])\n",
    "    annot_dfs[model] = pd.DataFrame(annot_dcts[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pval(pval, thresh1, thresh2, sens_pvals=None):\n",
    "    if pval < thresh2:\n",
    "        if sens_pvals:\n",
    "            for sens_pval in sens_pvals:\n",
    "                if sens_pval >= thresh2:\n",
    "                    return '**'\n",
    "            return '***'\n",
    "        else:\n",
    "            return '**'\n",
    "    elif pval < thresh1:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized[['owth_bin', 'owth_bin*Time_C']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_intvns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104d005-7806-4c2e-a94e-fa8444f26ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_primary:\n",
    "    #for outcome in ['ksu_beef', 'ksu_pork', 'ksu_chicken']:\n",
    "    sens_add_L = not add_L\n",
    "    for outcome in test_outcomes:\n",
    "        for intvn in test_intvns:\n",
    "            if outcome in restrictions and intvn not in restrictions[outcome]:\n",
    "                continue\n",
    "            \n",
    "            print('intvn: ', intvn)\n",
    "            \n",
    "            if 'assoc' in models_to_run:\n",
    "                print('Association: ')\n",
    "                assoc_pval, assoc_beta, assoc_se = run_primary_analyses(normalized.copy(), intvn, outcome, PS=False, \n",
    "                                     fit_method='GLSAR', model='assoc', \n",
    "                                     add_L=False, difference=do_differencing, include_time=include_time, \n",
    "                                     include_month=False, normalize=normalize, verbose=verbose)\n",
    "\n",
    "                print('beta, pval: ', assoc_beta, assoc_pval)\n",
    "                coef_dcts['assoc'][intvn_map[intvn]][outcome_map[outcome]] = assoc_beta\n",
    "                annot_dcts['assoc'][intvn_map[intvn]][outcome_map[outcome]] = eval_pval(assoc_pval, pval_thresh_1, pval_thresh_2)\n",
    "                se_dcts['assoc'][intvn_map[intvn]][outcome_map[outcome]] = assoc_se\n",
    "                pval_dcts['assoc'][intvn_map[intvn]][outcome_map[outcome]] = assoc_pval\n",
    "            \n",
    "            if 'contemp' in models_to_run:\n",
    "                print('Contemporaneous: ')\n",
    "                contemp_pval, contemp_beta, contemp_se = run_primary_analyses(normalized.copy(), intvn, outcome, PS=True, \n",
    "                                    fit_method='GLSAR', model='contemp', \n",
    "                                     add_L=add_L, difference=do_differencing, include_time=include_time, \n",
    "                                     include_month=include_month, normalize=normalize, verbose=verbose)\n",
    "\n",
    "                contemp_pval_sens1, _, _ = run_primary_analyses(normalized.copy(), intvn, outcome, PS=True, \n",
    "                                    fit_method='GLSAR', model='contemp', \n",
    "                                     add_L=sens_add_L, difference=do_differencing, include_time=include_time, \n",
    "                                     include_month=include_month, normalize=normalize, verbose=verbose)            \n",
    "\n",
    "                contemp_pval_sens2, _, _ = run_primary_analyses(normalized.copy(), intvn, outcome, PS=False, \n",
    "                                    fit_method='GLSAR', model='contemp', \n",
    "                                     add_L=add_L, difference=do_differencing, include_time=include_time, \n",
    "                                     include_month=include_month, normalize=normalize, verbose=verbose)   \n",
    "\n",
    "                print('beta, pval: ', contemp_beta, contemp_pval)\n",
    "                coef_dcts['contemp'][intvn_map[intvn]][outcome_map[outcome]] = contemp_beta\n",
    "                se_dcts['contemp'][intvn_map[intvn]][outcome_map[outcome]] = contemp_se\n",
    "                contemp_pvals_sens = [contemp_pval_sens1, contemp_pval_sens2]\n",
    "                print('contemp_pvals_sens: ', contemp_pvals_sens)\n",
    "                annot_dcts['contemp'][intvn_map[intvn]][outcome_map[outcome]] = eval_pval(contemp_pval, pval_thresh_1, pval_thresh_2, contemp_pvals_sens)\n",
    "                pval_dcts['contemp'][intvn_map[intvn]][outcome_map[outcome]] = contemp_pval\n",
    "            \n",
    "            if 'lagged' in models_to_run:\n",
    "                print('Lagged: ')\n",
    "                lagged_pval, lagged_beta, lagged_se = run_primary_analyses(normalized.copy(), intvn, outcome, PS=True, \n",
    "                                     fit_method='GLSAR', model='lagged', \n",
    "                                     add_L=add_L, difference=do_differencing, include_time=include_time, \n",
    "                                     include_month=include_month, normalize=normalize, verbose=verbose)\n",
    "\n",
    "                lagged_pval_sens1, _, _ = run_primary_analyses(normalized.copy(), intvn, outcome, PS=True, \n",
    "                                     fit_method='GLSAR', model='lagged', \n",
    "                                     add_L=sens_add_L, difference=do_differencing, include_time=include_time, \n",
    "                                     include_month=include_month, normalize=normalize, verbose=verbose)\n",
    "\n",
    "                lagged_pval_sens2, _, _ = run_primary_analyses(normalized.copy(), intvn, outcome, PS=False, \n",
    "                                     fit_method='GLSAR', model='lagged', \n",
    "                                     add_L=add_L, difference=do_differencing, include_time=include_time, \n",
    "                                     include_month=include_month, normalize=normalize, verbose=verbose)\n",
    "\n",
    "                print('beta, pval: ', lagged_beta, lagged_pval)\n",
    "                coef_dcts['lagged'][intvn_map[intvn]][outcome_map[outcome]] = lagged_beta\n",
    "                se_dcts['lagged'][intvn_map[intvn]][outcome_map[outcome]] = lagged_se\n",
    "                lagged_pvals_sens = [lagged_pval_sens1, lagged_pval_sens2]\n",
    "                print('lagged_pvals_sens: ', lagged_pvals_sens)\n",
    "                annot_dcts['lagged'][intvn_map[intvn]][outcome_map[outcome]] = eval_pval(lagged_pval, pval_thresh_1, pval_thresh_2, lagged_pvals_sens)\n",
    "                pval_dcts['lagged'][intvn_map[intvn]][outcome_map[outcome]] = lagged_pval\n",
    "\n",
    "            print('Bin: ')\n",
    "            if 'bin' in models_to_run:\n",
    "                if (outcome in binary_analysis_dct) and (intvn not in binary_analysis_dct[outcome]):\n",
    "                    continue\n",
    "\n",
    "                if intvn == 'all_docs':\n",
    "                    continue\n",
    "                    \n",
    "                bin_pval, bin_beta, bin_se = run_primary_analyses(normalized.copy(), intvn, outcome, PS=False, \n",
    "                                                          PS_logistic=False,\n",
    "                                     fit_method='GLSAR', model='bin', \n",
    "                                     add_L=add_L, difference=do_differencing_bin, include_time=include_time_bin, \n",
    "                                     include_month=include_month, duration_months=duration_months, \n",
    "                                                          normalize=normalize, verbose=verbose)\n",
    "\n",
    "                bin_pval_sens1, _, _ = run_primary_analyses(normalized.copy(), intvn, outcome, PS=False,\n",
    "                                                   PS_logistic=False,\n",
    "                                     fit_method='GLSAR', model='bin', \n",
    "                                     add_L=sens_add_L, difference=do_differencing_bin, include_time=include_time_bin, \n",
    "                                     include_month=include_month, duration_months=duration_months, normalize=normalize, verbose=verbose)\n",
    "\n",
    "                bin_pval_sens2, _, _ = run_primary_analyses(normalized.copy(), intvn, outcome, PS=False, \n",
    "                                    PS_logistic=False,\n",
    "                                     fit_method='GLSAR', model='bin', \n",
    "                                     add_L=add_L, difference=do_differencing_bin, include_time=include_time_bin, \n",
    "                                     include_month=include_month, duration_months=duration_months, normalize=normalize, verbose=verbose)\n",
    "\n",
    "                print('beta, pval: ', bin_beta, bin_pval)            \n",
    "                    \n",
    "                bin_pvals_sens = [bin_pval_sens1, bin_pval_sens2]\n",
    "                coef_dcts['bin'][intvn_map[intvn]][outcome_map[outcome]] = bin_beta\n",
    "                se_dcts['bin'][intvn_map[intvn]][outcome_map[outcome]] = bin_se\n",
    "                annot_dcts['bin'][intvn_map[intvn]][outcome_map[outcome]] = eval_pval(bin_pval, pval_thresh_1, pval_thresh_2, bin_pvals_sens)\n",
    "                pval_dcts['bin'][intvn_map[intvn]][outcome_map[outcome]] = bin_pval\n",
    "                print('bin_pvals_sens: ', bin_pvals_sens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffdd7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "min_val = np.inf\n",
    "max_val = -np.inf\n",
    "for model in ['assoc', 'contemp', 'lagged', 'bin']:\n",
    "    heat_dfs[model] = pd.DataFrame(coef_dcts[model])\n",
    "    annot_dfs[model] = pd.DataFrame(annot_dcts[model])\n",
    "    \n",
    "    if heat_dfs[model].min().min() < min_val:\n",
    "        min_val = heat_dfs[model].min().min()\n",
    "    if heat_dfs[model].max().max() > max_val:\n",
    "        max_val = heat_dfs[model].max().max()\n",
    "    \n",
    "\n",
    "model_maps = {'assoc': 'Association', 'contemp': 'Contemporaneous', 'lagged': 'Lagged',\n",
    "             'bin': 'Binary ({d} Months)'.format(d=duration_months)}\n",
    "    \n",
    "for model in ['assoc', 'contemp', 'lagged', 'bin']:    \n",
    "    plt.figure(figsize=(8, 6))  # Optional: Adjusts the size of the figure\n",
    "    cmap = sns.diverging_palette(h_neg=130, h_pos=10, s=99, l=55, sep=3, as_cmap=True)\n",
    "\n",
    "    ax = sns.heatmap(heat_dfs[model], annot=annot_dfs[model], center=0, fmt='s',\n",
    "                     cmap = cmap,\n",
    "                    annot_kws={\"size\": 25}, vmin=min_val, vmax=max_val, yticklabels=True, cbar=True)  # 'annot' annotates the boxes with the data values\n",
    "\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "    # Display the heatmap\n",
    "    plt.title(model_maps[model])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_se_df = pd.DataFrame(se_dcts['assoc'])\n",
    "assoc_coef_df = pd.DataFrame(coef_dcts['assoc'])\n",
    "assoc_pval_df = pd.DataFrame(pval_dcts['assoc'])\n",
    "\n",
    "lagged_se_df = pd.DataFrame(se_dcts['lagged'])\n",
    "lagged_coef_df = pd.DataFrame(coef_dcts['lagged'])\n",
    "lagged_pval_df = pd.DataFrame(pval_dcts['lagged'])\n",
    "\n",
    "contemp_se_df = pd.DataFrame(se_dcts['contemp'])\n",
    "contemp_coef_df = pd.DataFrame(coef_dcts['contemp'])\n",
    "contemp_pval_df = pd.DataFrame(pval_dcts['contemp'])\n",
    "\n",
    "bin_se_df = pd.DataFrame(se_dcts['bin'])\n",
    "bin_coef_df = pd.DataFrame(coef_dcts['bin'])\n",
    "bin_pval_df = pd.DataFrame(pval_dcts['bin'])\n",
    "\n",
    "for col in lagged_se_df.columns:\n",
    "    assoc_se_df = assoc_se_df.rename(columns={col:col+'_se'})\n",
    "    assoc_coef_df = assoc_coef_df.rename(columns={col:col+'_pe'})\n",
    "    assoc_pval_df = assoc_pval_df.rename(columns={col:col+'_pval'})\n",
    "    \n",
    "    lagged_se_df = lagged_se_df.rename(columns={col:col+'_se'})\n",
    "    lagged_coef_df = lagged_coef_df.rename(columns={col:col+'_pe'})\n",
    "    lagged_pval_df = lagged_pval_df.rename(columns={col:col+'_pval'})\n",
    "\n",
    "    contemp_se_df = contemp_se_df.rename(columns={col:col+'_se'})\n",
    "    contemp_coef_df = contemp_coef_df.rename(columns={col:col+'_pe'})\n",
    "    contemp_pval_df = contemp_pval_df.rename(columns={col:col+'_pval'})\n",
    "\n",
    "    bin_se_df = bin_se_df.rename(columns={col:col+'_se'})\n",
    "    bin_coef_df = bin_coef_df.rename(columns={col:col+'_pe'})\n",
    "    bin_pval_df = bin_pval_df.rename(columns={col:col+'_pval'})\n",
    "    \n",
    "assoc_df = pd.concat([assoc_se_df, assoc_coef_df, assoc_pval_df], axis=1)\n",
    "lagged_df = pd.concat([lagged_se_df, lagged_coef_df, lagged_pval_df], axis=1)\n",
    "contemp_df = pd.concat([contemp_se_df, contemp_coef_df, contemp_pval_df], axis=1)\n",
    "bin_df = pd.concat([bin_se_df, bin_coef_df, bin_pval_df], axis=1)\n",
    "\n",
    "assoc_df.index.name = 'Outcome'\n",
    "lagged_df.index.name = 'Outcome'\n",
    "contemp_df.index.name = 'Outcome'\n",
    "bin_df.index.name = 'Outcome'\n",
    "\n",
    "lagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa253fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "save=True \n",
    "if save:\n",
    "    prefix = '../results/' + run_name + '_' + units + '_' + str(datetime.now()).replace(' ', '_').replace(':', '_') + '/'\n",
    "    os.makedirs(prefix)\n",
    "    assoc_df.to_csv(prefix + 'assoc_df_{u}_all_primary.csv'.format(u=units))    \n",
    "    lagged_df.to_csv(prefix + 'lagged_df_{u}_all_primary.csv'.format(u=units))\n",
    "    contemp_df.to_csv(prefix + 'contemp_df_{u}_all_primary.csv'.format(u=units))\n",
    "    bin_df.to_csv(prefix + 'bin_df_{u}_all_primary.csv'.format(u=units))\n",
    "    \n",
    "    annot_dfs['assoc'].to_csv(prefix + 'annot_assoc_df_{u}_all_primary.csv'.format(u=units))\n",
    "    annot_dfs['contemp'].to_csv(prefix + 'annot_contemp_df_{u}_all_primary.csv'.format(u=units))\n",
    "    annot_dfs['lagged'].to_csv(prefix + 'annot_lagged_df_{u}_all_primary.csv'.format(u=units))\n",
    "    annot_dfs['bin'].to_csv(prefix + 'annot_bin_df_{u}_all_primary.csv'.format(u=units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
